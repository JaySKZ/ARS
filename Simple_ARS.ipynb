{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://towardsdatascience.com/introduction-to-augmented-random-search-d8d7b55309bd\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import safety_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.n_iter = 50\n",
    "        self.episode_length = 1000\n",
    "        self.step_size = 0.02\n",
    "        self.n_directions = 16\n",
    "        self.n_best_directions = 16\n",
    "        assert self.n_best_directions <= self.n_directions\n",
    "        self.noise = 0.03\n",
    "        self.seed = 1\n",
    "        self.env_name = 'Safexp-PointGoal0-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize states\n",
    "class Normalizer():\n",
    "    \n",
    "    def __init__(self, n_inputs):\n",
    "        self.n = np.zeros(n_inputs)\n",
    "        self.mean = np.zeros(n_inputs)\n",
    "        self.mean_diff = np.zeros(n_inputs)\n",
    "        self.var = np.zeros(n_inputs)\n",
    "        \n",
    "    def observe(self, x):\n",
    "        self.n+=1\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x-self.mean)/self.n\n",
    "        self.mean_diff += (x-last_mean)*(x-self.mean)\n",
    "        self.var = (self.mean_diff/self.n).clip(min=1e-2)\n",
    "        \n",
    "    def normalise(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs-obs_mean)/obs_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    \n",
    "    def __init__(self,input_size,output_size):\n",
    "        # initiate weights matrix\n",
    "        self.theta = np.zeros((output_size,input_size))             \n",
    "    \n",
    "    def evaluate(self,inputs,delta=None,direction=None):\n",
    "        if direction is None:\n",
    "            return (self.theta).dot(inputs)\n",
    "        elif direction == 'positive':\n",
    "            return (self.theta + hp.noise * delta).dot(inputs)\n",
    "        else:\n",
    "            return (self.theta - hp.noise * delta).dot(inputs)\n",
    "        \n",
    "    def sample_deltas(self):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.n_directions)]\n",
    "    \n",
    "    def update(self, rollouts , sigma_r):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        \n",
    "        #Approx Grad. Descent\n",
    "        for rpos,rneg,d in rollouts:\n",
    "            step+= (rpos - rneg)*d\n",
    "            \n",
    "        self.theta+= hp.step_size/(hp.n_best_directions*sigma_r)*step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore policy on one specific direction over one episode\n",
    "\n",
    "def explore(env, normalizer, policy, direction = None , delta = None):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    num_plays = 0\n",
    "    sum_rewards = 0\n",
    "    \n",
    "    while not done and num_plays < hp.episode_length:\n",
    "        normalizer.observe(state)\n",
    "        state = normalizer.normalise(state)\n",
    "        action = policy.evaluate(state,delta,direction)\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        reward = max(min(reward,1),-1)\n",
    "        sum_rewards += reward\n",
    "        num_plays += 1\n",
    "        \n",
    "    return sum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "def train(env, policy, normalizer, hp):\n",
    "    for i in range(hp.n_iter):\n",
    "\n",
    "        # Initialize deltas/pertubations for adjusting the weights\n",
    "        deltas = policy.sample_deltas()\n",
    "        positive_rewards = [0] * hp.n_directions\n",
    "        negative_rewards = [0] * hp.n_directions\n",
    "\n",
    "        #Getting the positive rewards in positive direction\n",
    "        for k in range(hp.n_directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy, direction=\"positive\", delta=deltas[k])\n",
    "\n",
    "        #Getting the negative rewards in negative direction\n",
    "        for k in range(hp.n_directions):\n",
    "            negative_rewards[k] = explore(env, normalizer, policy, direction=\"negative\", delta=deltas[k])\n",
    "\n",
    "        #Gathering all positive/negative rewards to compute the standard deviation of these results\n",
    "        all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "        sigma_r = all_rewards.std()\n",
    "\n",
    "        #Sorting the rollouts by the max(rpos,rneg) and selecting the best directions\n",
    "        scores = { k:[max(r_pos,r_pos)]  for k,(r_pos,r_neg) in enumerate(zip(positive_rewards,negative_rewards))}\n",
    "        order = sorted(scores.keys(), key = lambda x: scores[x])[:hp.n_best_directions]\n",
    "        rollouts = [[positive_rewards[k],negative_rewards[k],deltas[k]] for k in order]\n",
    "\n",
    "        #Update policy\n",
    "        policy.update(rollouts,sigma_r)\n",
    "        \n",
    "        env.reset() \n",
    "        env.render()\n",
    "\n",
    "        #Printing the final reward of the policy after the update\n",
    "        reward_evaluation = explore(env, normalizer, policy)\n",
    "        print('Step: ',i,' Reward :', reward_evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for video\n",
    "\n",
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzhou/.local/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n",
      "Step:  0  Reward : 0.2680458928672226\n",
      "Step:  1  Reward : 0.06072746605757895\n",
      "Step:  2  Reward : 0.6100334672317146\n",
      "Step:  3  Reward : 0.13244796931052016\n",
      "Step:  4  Reward : 0.12363487182038468\n"
     ]
    }
   ],
   "source": [
    "#Running\n",
    "\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "\n",
    "hp = HyperParameters()\n",
    "np.random.seed(hp.seed)\n",
    "env = gym.make(hp.env_name)\n",
    "env = wrappers.Monitor(env, monitor_dir, force=True)\n",
    "nb_inputs = env.observation_space.shape[0]\n",
    "nb_outputs = env.action_space.shape[0]\n",
    "policy = Policy(nb_inputs, nb_outputs)\n",
    "normalizer = Normalizer(nb_inputs)\n",
    "train(env, policy, normalizer, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}